# Serlin-Transformer By Ryan Crepa
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
import json
import sqlite3
import pickle
import hashlib
from collections import defaultdict, deque, OrderedDict
import datetime
import math

class LongTermMemory:
    """é•¿æœŸè®°å¿†ç³»ç»Ÿ"""
    
    def __init__(self, db_path="memory.db"):
        self.db_path = db_path
        self.init_database()
        
    def init_database(self):
        """åˆå§‹åŒ–è®°å¿†æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ç”¨æˆ·ä¿¡æ¯è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS users (
                user_id TEXT PRIMARY KEY,
                personality_profile TEXT,
                preferences TEXT,
                created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # å¯¹è¯è®°å¿†è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS conversations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT,
                input_text TEXT,
                response_text TEXT,
                sentiment REAL,
                topics TEXT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (user_id) REFERENCES users (user_id)
            )
        ''')
        
        # çŸ¥è¯†è®°å¿†è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS knowledge (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                key_text TEXT UNIQUE,
                value_text TEXT,
                confidence REAL,
                source TEXT,
                last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # è‡ªæˆ‘åæ€è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS reflections (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                conversation_id INTEGER,
                reflection_text TEXT,
                improvement_suggestions TEXT,
                quality_score REAL,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (conversation_id) REFERENCES conversations (id)
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def store_conversation(self, user_id, input_text, response_text, sentiment, topics):
        """å­˜å‚¨å¯¹è¯è®°å½•"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
    
        # ç¡®ä¿ç”¨æˆ·å­˜åœ¨
        cursor.execute('INSERT OR IGNORE INTO users (user_id) VALUES (?)', (user_id,))
    
        # å¤„ç†æƒ…æ„Ÿå€¼
        if isinstance(sentiment, torch.Tensor):
            sentiment_value = sentiment.mean().item()
        else:
            sentiment_value = float(sentiment)
    
        cursor.execute('''
            INSERT INTO conversations (user_id, input_text, response_text, sentiment, topics)
            VALUES (?, ?, ?, ?, ?)
        ''', (user_id, input_text, response_text, sentiment_value, json.dumps(topics)))
    
        conn.commit()
        conn.close()
    
    def get_user_history(self, user_id, limit=10):
        """è·å–ç”¨æˆ·å¯¹è¯å†å²"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT input_text, response_text, sentiment, topics, timestamp
            FROM conversations 
            WHERE user_id = ? 
            ORDER BY timestamp DESC 
            LIMIT ?
        ''', (user_id, limit))
        
        results = cursor.fetchall()
        conn.close()
        
        return [{
            'input': row[0],
            'response': row[1],
            'sentiment': row[2],
            'topics': json.loads(row[3]),
            'timestamp': row[4]
        } for row in results]
    
    def store_knowledge(self, key, value, confidence=1.0, source="conversation"):
        """å­˜å‚¨çŸ¥è¯†"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO knowledge (key_text, value_text, confidence, source, last_accessed)
            VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP)
        ''', (key, value, confidence, source))
        
        conn.commit()
        conn.close()
    
    def retrieve_knowledge(self, key):
        """æ£€ç´¢çŸ¥è¯†"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT value_text, confidence FROM knowledge 
            WHERE key_text = ? AND confidence > 0.5
            ORDER BY confidence DESC
        ''', (key,))
        
        result = cursor.fetchone()
        conn.close()
        
        if result:
            self.update_knowledge_access(key)
            return result[0], result[1]
        return None, 0.0
    
    def update_knowledge_access(self, key):
        """æ›´æ–°çŸ¥è¯†è®¿é—®æ—¶é—´"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            UPDATE knowledge SET last_accessed = CURRENT_TIMESTAMP 
            WHERE key_text = ?
        ''', (key,))
        
        conn.commit()
        conn.close()
    
    def store_reflection(self, conversation_id, reflection, suggestions, quality_score):
        """å­˜å‚¨è‡ªæˆ‘åæ€"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO reflections (conversation_id, reflection_text, improvement_suggestions, quality_score)
            VALUES (?, ?, ?, ?)
        ''', (conversation_id, reflection, suggestions, quality_score))
        
        conn.commit()
        conn.close()

class KnowledgeBase:
    """çŸ¥è¯†åº“ç³»ç»Ÿ"""
    
    def __init__(self, memory_system):
        self.memory = memory_system
        self.domain_knowledge = self.load_domain_knowledge()
    
    def load_domain_knowledge(self):
        """åŠ è½½é¢†åŸŸçŸ¥è¯†"""
        return {
            "technology": {
                "python": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥ç®€æ´æ˜“è¯»è‘—ç§°",
                "ai": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›é€ æ™ºèƒ½æœºå™¨",
                "machine learning": "æœºå™¨å­¦ä¹ æ˜¯AIçš„å­é¢†åŸŸï¼Œè®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ "
            },
            "entertainment": {
                "movies": "ç”µå½±æ˜¯ä¸€ç§é‡è¦çš„å¨±ä¹å½¢å¼",
                "music": "éŸ³ä¹å¯ä»¥è¡¨è¾¾æƒ…æ„Ÿå’Œåˆ›é€ æ°›å›´",
                "games": "æ¸¸æˆå¯ä»¥æä¾›å¨±ä¹å’ŒæŒ‘æˆ˜"
            }
        }
    
    def query_knowledge(self, query, domain=None):
        """æŸ¥è¯¢çŸ¥è¯†"""
        # å…ˆä»é•¿æœŸè®°å¿†æŸ¥è¯¢
        knowledge, confidence = self.memory.retrieve_knowledge(query)
        if knowledge:
            return knowledge, confidence
        
        # ä»é¢†åŸŸçŸ¥è¯†æŸ¥è¯¢
        if domain and domain in self.domain_knowledge:
            if query in self.domain_knowledge[domain]:
                return self.domain_knowledge[domain][query], 0.8
        
        # ä»é€šç”¨é¢†åŸŸçŸ¥è¯†æŸ¥è¯¢
        for domain_knowledge in self.domain_knowledge.values():
            if query in domain_knowledge:
                return domain_knowledge[query], 0.7
        
        return None, 0.0
    
    def learn_from_conversation(self, user_input, response):
        """ä»å¯¹è¯ä¸­å­¦ä¹ æ–°çŸ¥è¯†"""
        words = user_input.lower().split()
        for word in words:
            if len(word) > 3:
                if word in response.lower():
                    self.memory.store_knowledge(word, response, 0.6, "conversation_learning")

class MultiTurnContext:
    """å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†"""
    
    def __init__(self, max_context_length=10):
        self.max_context_length = max_context_length
        self.conversation_context = deque(maxlen=max_context_length)
        self.current_topics = set()
    
    def add_turn(self, user_input, ai_response, sentiment, extracted_topics):
        """æ·»åŠ ä¸€è½®å¯¹è¯åˆ°ä¸Šä¸‹æ–‡"""
        turn = {
            'user_input': user_input,
            'ai_response': ai_response,
            'sentiment': sentiment,
            'topics': extracted_topics,
            'timestamp': datetime.datetime.now()
        }
        self.conversation_context.append(turn)
        
        # æ›´æ–°å½“å‰è¯é¢˜
        self.current_topics.update(extracted_topics)
        # é™åˆ¶è¯é¢˜æ•°é‡
        if len(self.current_topics) > 8:
            self.current_topics = set(list(self.current_topics)[-8:])
    
    def get_context_text(self):
        """è·å–ä¸Šä¸‹æ–‡æ–‡æœ¬"""
        if not self.conversation_context:
            return ""
        
        context_texts = []
        for turn in list(self.conversation_context)[-3:]:
            context_texts.extend([turn['user_input'], turn['ai_response']])
        
        return " ".join(context_texts)
    
    def get_recent_topics(self):
        """è·å–æœ€è¿‘çš„è¯é¢˜"""
        return list(self.current_topics)[-5:]

class PersonalityAdaptation:
    """ä¸ªæ€§åŒ–é€‚åº”ç³»ç»Ÿ"""
    
    def __init__(self, memory_system):
        self.memory = memory_system
        self.user_profiles = {}
    
    def get_user_profile(self, user_id):
        """è·å–ç”¨æˆ·ä¸ªæ€§ç”»åƒ"""
        conn = sqlite3.connect(self.memory.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT personality_profile, preferences FROM users WHERE user_id = ?', (user_id,))
        result = cursor.fetchone()
        conn.close()
        
        if result and result[0]:
            return json.loads(result[0]), json.loads(result[1])
        else:
            # é»˜è®¤ä¸ªæ€§ç”»åƒ
            default_profile = {
                "formality": 0.5,
                "humor_level": 0.3,
                "detail_level": 0.6,
                "empathy_level": 0.7,
                "curiosity_level": 0.5
            }
            default_prefs = {
                "preferred_topics": [],
                "avoided_topics": [],
                "communication_style": "balanced"
            }
            return default_profile, default_prefs
    
    def update_user_profile(self, user_id, user_input, response, sentiment):
        """åŸºäºå¯¹è¯æ›´æ–°ç”¨æˆ·ç”»åƒ"""
        profile, prefs = self.get_user_profile(user_id)
        
        # åˆ†æç”¨æˆ·è¾“å…¥ç‰¹å¾æ¥æ›´æ–°ç”»åƒ
        input_lower = user_input.lower()
        
        # æ›´æ–°æ­£å¼ç¨‹åº¦
        if any(word in input_lower for word in ['æ‚¨å¥½', 'è¯·é—®', 'éº»çƒ¦']):
            profile["formality"] = min(1.0, profile["formality"] + 0.1)
        elif any(word in input_lower for word in ['å˜¿', 'å—¨', 'å“ˆå“ˆ']):
            profile["formality"] = max(0.0, profile["formality"] - 0.1)
        
        # æ›´æ–°å¹½é»˜æ„Ÿ
        if any(word in input_lower for word in ['ç¬‘è¯', 'æç¬‘', 'å¹½é»˜']):
            profile["humor_level"] = min(1.0, profile["humor_level"] + 0.15)
        
        # æ›´æ–°æƒ…æ„Ÿæ°´å¹³
        if sentiment > 0.6:
            profile["empathy_level"] = min(1.0, profile["empathy_level"] + 0.05)
        
        # ä¿å­˜æ›´æ–°åçš„ç”»åƒ
        conn = sqlite3.connect(self.memory.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO users (user_id, personality_profile, preferences)
            VALUES (?, ?, ?)
        ''', (user_id, json.dumps(profile), json.dumps(prefs)))
        
        conn.commit()
        conn.close()
        
        return profile

class SelfReflection:
    """è‡ªæˆ‘åæ€ç³»ç»Ÿ"""
    
    def __init__(self, memory_system):
        self.memory = memory_system
    
    def analyze_response_quality(self, user_input, ai_response, sentiment):
        """åˆ†æå›åº”è´¨é‡"""
        quality_score = 0.5  # åŸºç¡€åˆ†
        
        # åŸºäºé•¿åº¦è¯„ä¼°
        if len(ai_response.split()) >= 5 and len(ai_response.split()) <= 50:
            quality_score += 0.2
        
        # å¤„ç†æƒ…æ„Ÿå€¼
        if isinstance(sentiment, torch.Tensor):
            sentiment_value = sentiment.mean().item()
        else:
            sentiment_value = float(sentiment)
        
        # åŸºäºæƒ…æ„Ÿä¸€è‡´æ€§
        if sentiment_value > 0.3 and sentiment_value < 0.8:
            quality_score += 0.1
        
        # åŸºäºé—®é¢˜ç›¸å…³æ€§
        user_words = set(user_input.lower().split())
        response_words = set(ai_response.lower().split())
        common_words = user_words.intersection(response_words)
        if len(common_words) > 0:
            quality_score += 0.2
        
        return min(1.0, quality_score)
    
    def generate_improvement_suggestions(self, user_input, ai_response, quality_score):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []
        
        if quality_score < 0.6:
            if len(ai_response.split()) < 3:
                suggestions.append("å›åº”è¿‡äºç®€çŸ­ï¼Œå¯ä»¥æä¾›æ›´å¤šç»†èŠ‚")
            elif len(ai_response.split()) > 60:
                suggestions.append("å›åº”å¯èƒ½è¿‡é•¿ï¼Œè€ƒè™‘æ›´ç®€æ´è¡¨è¾¾")
            
            if "?" in user_input and "?" not in ai_response:
                suggestions.append("ç”¨æˆ·çš„é—®é¢˜å¯èƒ½éœ€è¦æ›´ç›´æ¥çš„ç­”æ¡ˆ")
        
        return suggestions
    
    def reflect_on_conversation(self, conversation_id, user_input, ai_response, sentiment):
        """å¯¹å¯¹è¯è¿›è¡Œåæ€"""
        quality_score = self.analyze_response_quality(user_input, ai_response, sentiment)
        suggestions = self.generate_improvement_suggestions(user_input, ai_response, quality_score)
        
        reflection_text = f"å›åº”è´¨é‡è¯„åˆ†: {quality_score:.2f}. "
        if suggestions:
            reflection_text += "æ”¹è¿›å»ºè®®: " + "; ".join(suggestions)
        else:
            reflection_text += "è¿™æ¬¡å›åº”è´¨é‡ä¸é”™ã€‚"
        
        # å­˜å‚¨åæ€ç»“æœ
        self.memory.store_reflection(conversation_id, reflection_text, 
                                   json.dumps(suggestions), quality_score)
        
        return reflection_text, suggestions, quality_score

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç  - é€‚é…batch_first"""
    
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        # æ³¨å†Œä¸ºbufferï¼Œä¸å‚ä¸è®­ç»ƒ
        self.register_buffer('pe', pe.unsqueeze(0))  # [1, max_len, d_model]
    
    def forward(self, x):
        # xå½¢çŠ¶: [batch_size, seq_len, d_model]
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len, :]

class TransformerThinkingLayer(nn.Module):
    """Transformeræ€è€ƒå±‚ - é€‚é…batch_first"""
    
    def __init__(self, d_model, nhead, num_layers, think_steps=3):
        super(TransformerThinkingLayer, self).__init__()
        self.d_model = d_model
        self.think_steps = think_steps
        
        # æ€è€ƒTransformerå±‚ - è®¾ç½®batch_first=True
        self.thinking_transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=d_model, 
                nhead=nhead,
                batch_first=True  # æ·»åŠ è¿™ä¸ªå‚æ•°
            ),
            num_layers=num_layers
        )
        
        # æ€è€ƒèåˆå±‚
        self.thought_fusion = nn.Linear(d_model * think_steps, d_model)
        
        # å±‚å½’ä¸€åŒ–
        self.layer_norm = nn.LayerNorm(d_model)
    
    def forward(self, context, knowledge_vector=None, memory_vector=None):
        # contextå½¢çŠ¶: [batch_size, seq_len, d_model]
        batch_size, seq_len, d_model = context.size()
        
        # å¤šæ­¥æ€è€ƒè¿‡ç¨‹
        thoughts = []
        current_thought = context
        
        for step in range(self.think_steps):
            # åº”ç”¨Transformeræ€è€ƒ
            thought_output = self.thinking_transformer(current_thought)
            
            # é›†æˆçŸ¥è¯†å’Œè®°å¿†ï¼ˆå¦‚æœæä¾›ï¼‰
            if knowledge_vector is not None:
                # knowledge_vector: [batch_size, d_model]
                knowledge_expanded = knowledge_vector.unsqueeze(1).expand(-1, seq_len, -1)
                thought_output = thought_output + knowledge_expanded
            
            if memory_vector is not None:
                # memory_vector: [batch_size, d_model]
                memory_expanded = memory_vector.unsqueeze(1).expand(-1, seq_len, -1)
                thought_output = thought_output + memory_expanded
            
            thoughts.append(thought_output)
            current_thought = thought_output
        
        # èåˆæ‰€æœ‰æ€è€ƒæ­¥éª¤
        if len(thoughts) > 1:
            # æ²¿ç€ç‰¹å¾ç»´åº¦æ‹¼æ¥
            thought_cat = torch.cat(thoughts, dim=-1)  # [batch_size, seq_len, d_model * think_steps]
            final_thought = self.thought_fusion(thought_cat)
        else:
            final_thought = thoughts[0]
        
        # å±‚å½’ä¸€åŒ–
        final_thought = self.layer_norm(final_thought)
        
        return final_thought  # [batch_size, seq_len, d_model]


class TransformerDialogueAI(nn.Module):
    """åŸºäºTransformerçš„å¯¹è¯AI - ä¿®å¤batch_firstè­¦å‘Š"""
    
    def __init__(self, vocab_size,idx2word, d_model=512, nhead=8, 
                 num_encoder_layers=6, num_decoder_layers=6,
                 think_steps=3, max_length=100, dropout=0.1):
        super(TransformerDialogueAI, self).__init__()
        
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.max_length = max_length
        
        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_len=max_length)
        self.idx2word = idx2word  # æ·»åŠ è¿™ä¸ªå±æ€§
        # Transformerç¼–ç å™¨ - è®¾ç½®batch_first=True
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead,
            dropout=dropout,
            batch_first=True  # æ·»åŠ è¿™ä¸ªå‚æ•°
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)
        
        # Transformeræ€è€ƒå±‚
        self.thinking_layer = TransformerThinkingLayer(
            d_model=d_model,
            nhead=nhead,
            num_layers=2,
            think_steps=think_steps
        )
        
        # Transformerè§£ç å™¨ - è®¾ç½®batch_first=True
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=d_model,
            nhead=nhead,
            dropout=dropout,
            batch_first=True  # æ·»åŠ è¿™ä¸ªå‚æ•°
        )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)
        
        # è¾“å‡ºå±‚
        self.output_projection = nn.Linear(d_model, vocab_size)
        
        # ä¸ªæ€§é€‚é…å™¨
        self.personality_adapter = nn.Linear(d_model + 5, d_model)
        
        # æƒ…æ„Ÿå’Œä¸»é¢˜åˆ†æ
        self.sentiment_analysis = nn.Linear(d_model, 3)
        self.topic_analysis = nn.Linear(d_model, 10)
        
        # åˆå§‹åŒ–å‚æ•°
        self.init_weights()
    
    def init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        initrange = 0.1
        self.embedding.weight.data.uniform_(-initrange, initrange)
        self.output_projection.bias.data.zero_()
        self.output_projection.weight.data.uniform_(-initrange, initrange)
    
    def encode(self, src, src_mask=None):
        """ç¼–ç è¾“å…¥åºåˆ—"""
        # srcå½¢çŠ¶: [batch_size, seq_len]
        src_embedded = self.embedding(src) * math.sqrt(self.d_model)
        src_embedded = self.pos_encoder(src_embedded)
        
        # ç¼–ç å™¨æœŸæœ›è¾“å…¥: [batch_size, seq_len, d_model]
        memory = self.encoder(src_embedded, src_mask)
        return memory  # [batch_size, seq_len, d_model]
    
    def thinking_process(self, memory, knowledge_vector=None, memory_vector=None, personality_vector=None):
        """æ€è€ƒè¿‡ç¨‹"""
        # memoryå½¢çŠ¶: [batch_size, seq_len, d_model]
        
        # åº”ç”¨æ€è€ƒå±‚
        thought_memory = self.thinking_layer(memory, knowledge_vector, memory_vector)
        
        # ä¸ªæ€§é€‚é…
        if personality_vector is not None:
            batch_size, seq_len, d_model = thought_memory.size()
            personality_expanded = personality_vector.unsqueeze(1).expand(-1, seq_len, -1)
            thought_with_personality = torch.cat([thought_memory, personality_expanded], dim=-1)
            thought_memory = torch.tanh(self.personality_adapter(thought_with_personality))
        
        return thought_memory  # [batch_size, seq_len, d_model]
    
    def decode(self, tgt, memory, tgt_mask=None, memory_mask=None):
        """è§£ç ç”Ÿæˆå›åº”"""
        # tgtå½¢çŠ¶: [batch_size, seq_len]
        # memoryå½¢çŠ¶: [batch_size, seq_len, d_model]
        
        tgt_embedded = self.embedding(tgt) * math.sqrt(self.d_model)
        tgt_embedded = self.pos_encoder(tgt_embedded)
        
        # è§£ç å™¨æœŸæœ›è¾“å…¥: [batch_size, seq_len, d_model]
        output = self.decoder(tgt_embedded, memory, tgt_mask=tgt_mask, memory_mask=memory_mask)
        return self.output_projection(output)  # [batch_size, seq_len, vocab_size]
    
    def forward(self, src, tgt=None, knowledge_vector=None, 
            memory_vector=None, personality_vector=None, teacher_forcing_ratio=0.5):
    
        print(f"\n=== å‰å‘ä¼ æ’­è°ƒè¯• ===")
        print(f"è¾“å…¥srcå½¢çŠ¶: {src.shape}")
    
        batch_size = src.size(0)
        device = src.device
    
        # ç¼–ç è¾“å…¥
        memory = self.encode(src)
        print(f"ç¼–ç åmemoryå½¢çŠ¶: {memory.shape}")
    
        # æ€è€ƒè¿‡ç¨‹
        thought_memory = self.thinking_process(memory, knowledge_vector, memory_vector, personality_vector)
        print(f"æ€è€ƒåmemoryå½¢çŠ¶: {thought_memory.shape}")
    
        # è§£ç 
        if tgt is not None:
            print("è®­ç»ƒæ¨¡å¼")
            tgt_len = tgt.size(1)
            tgt_input = tgt[:, :-1]
            tgt_output = tgt[:, 1:]
        
            # åˆ›å»ºç›®æ ‡åºåˆ—æ©ç 
            tgt_mask = self.generate_square_subsequent_mask(tgt_input.size(1)).to(device)
        
            # è§£ç 
            decoder_output = self.decode(tgt_input, thought_memory, 
                                       tgt_mask=tgt_mask, memory_mask=None)
        
            # è½¬ç½®è¾“å‡ºä»¥åŒ¹é…ç›®æ ‡å½¢çŠ¶
            decoder_output = decoder_output.transpose(0, 1)
        else:
            print("æ¨ç†æ¨¡å¼ - ç”Ÿæˆå›åº”")
            # ä¿®æ”¹è¿™é‡Œï¼šæ¥æ”¶ç”Ÿæˆçš„tokenåºåˆ—
            decoder_output, generated_tokens = self.generate_autoregressive(thought_memory, batch_size, device)
            print(f"ç”Ÿæˆçš„tokenåºåˆ—: {generated_tokens}")
            tgt_output = None
    
        # åˆ†ææƒ…æ„Ÿå’Œä¸»é¢˜
        context_representation = thought_memory[:, 0, :]
        sentiment = self.sentiment_analysis(context_representation)
        topics = self.topic_analysis(context_representation)
    
        return {
            'output': decoder_output,
            'sentiment': sentiment,
            'topics': topics,
            'memory': thought_memory,
            'generated_tokens': generated_tokens if tgt is None else None  # æ·»åŠ ç”Ÿæˆçš„token
        }
    
    def generate_autoregressive(self, memory, batch_size, device):
        """æ”¹è¿›çš„è‡ªå›å½’ç”Ÿæˆ - é˜²æ­¢PADæ³›æ»¥"""
        print(f"=== ç”Ÿæˆè¿‡ç¨‹è°ƒè¯• ===")
    
        # åˆå§‹åŒ–ä¸ºSOSæ ‡è®°
        tgt = torch.ones(batch_size, 1, dtype=torch.long).to(device)
    
        outputs = []
        generated_tokens = []
    
        # ç”Ÿæˆå‚æ•°
        temperature = 0.9
        top_k = 30
        repetition_penalty = 1.2
    
        for i in range(min(20, self.max_length)):  # é™åˆ¶ç”Ÿæˆé•¿åº¦
            # åˆ›å»ºç›®æ ‡åºåˆ—æ©ç 
            tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(device)
        
            # è§£ç 
            output = self.decode(tgt, memory, tgt_mask=tgt_mask, memory_mask=None)
        
            # è·å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹
            next_word_logits = output[:, -1, :]
        
            # åº”ç”¨é‡å¤æƒ©ç½š
            for token in set(generated_tokens):
                next_word_logits[0, token] /= repetition_penalty
        
            # ç¦æ­¢ç”ŸæˆPADå’ŒSOSä½œä¸ºå†…å®¹
            next_word_logits[0, 0] = -float('Inf')  # PAD
            next_word_logits[0, 1] = -float('Inf')  # SOS
            next_word_logits[0, 3] = -float('Inf')  # UNK
        
            # æ¸©åº¦é‡‡æ · + top-k
            if temperature > 0:
                next_word_logits = next_word_logits / temperature
            
                # top-kè¿‡æ»¤
                if top_k > 0:
                    indices_to_remove = next_word_logits < torch.topk(next_word_logits, top_k)[0][..., -1, None]
                    next_word_logits[indices_to_remove] = -float('Inf')
            
                probabilities = torch.softmax(next_word_logits, dim=-1)
                next_word = torch.multinomial(probabilities, num_samples=1)
            else:
                next_word = next_word_logits.argmax(dim=-1, keepdim=True)
        
            word = self.idx2word.get(next_word.item(), '<UNK>')
            print(f"æ—¶é—´æ­¥ {i}: ç”Ÿæˆ '{word}' (ç´¢å¼•: {next_word.item()})")
        
            # å¦‚æœç”ŸæˆEOSï¼Œæå‰åœæ­¢
            if next_word.item() == 2:  # EOS
                print("ç”ŸæˆEOSï¼Œåœæ­¢")
                break
        
            # å¦‚æœè¿ç»­ç”Ÿæˆå¤ªå¤šæ— æ•ˆæ ‡è®°ï¼Œæå‰åœæ­¢
            if word in ['<PAD>', '<SOS>', '<UNK>']:
                if generated_tokens.count(next_word.item()) > 3:
                    print("æ£€æµ‹åˆ°æ— æ•ˆæ ‡è®°æ³›æ»¥ï¼Œæå‰åœæ­¢")
                    break
        
            # æ·»åŠ åˆ°åºåˆ—
            tgt = torch.cat([tgt, next_word], dim=1)
            outputs.append(output[:, -1:, :])
            generated_tokens.append(next_word.item())
    
        if outputs:
            final_output = torch.cat(outputs, dim=1)
            print(f"ç”Ÿæˆçš„tokenåºåˆ—: {generated_tokens}")
            return final_output, generated_tokens
        else:
            return torch.zeros(batch_size, 1, self.vocab_size).to(device), []
    def generate_square_subsequent_mask(self, sz):
        """ç”Ÿæˆåºåˆ—æ©ç """
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

class DialogueDataProcessor:
    """å¯¹è¯æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        self.word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}
        self.idx2word = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}
        self.vocab_size = 4
        
    def build_vocab(self, dialogues, min_freq=1):
        """æ„å»ºè¯æ±‡è¡¨"""
        word_freq = defaultdict(int)
    
        for dialogue in dialogues:
            for text in [dialogue['input'], dialogue['output']]:
                words = self.tokenize(text)
                for word in words:
                    word_freq[word] += 1
    
        print(f"å‘ç° {len(word_freq)} ä¸ªä¸åŒçš„è¯")
    
        # æ·»åŠ ç‰¹æ®Šæ ‡è®°
        special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']
        for token in special_tokens:
            self.word2idx[token] = len(self.word2idx)
            self.idx2word[len(self.idx2word)] = token
    
        # æ·»åŠ æ‰€æœ‰è¯
        for word, freq in word_freq.items():
            if freq >= min_freq and word not in self.word2idx:
                self.word2idx[word] = len(self.word2idx)
                self.idx2word[len(self.idx2word)] = word
    
        self.vocab_size = len(self.word2idx)
        print(f"è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼Œå¤§å°: {self.vocab_size}")
        print(f"ç¤ºä¾‹è¯æ±‡: {list(self.word2idx.keys())[:20]}")
    
        return self.vocab_size
    
    def auto_expand_vocab(self, text, min_freq=1):
        """è‡ªåŠ¨æ‰©å±•è¯æ±‡è¡¨ - ä¿®å¤ç‰ˆ"""
        new_words = []
        words = self.tokenize(text)

        for word in words:
            # è·³è¿‡ç©ºè¯å’Œç‰¹æ®Šæ ‡è®°
            if not word.strip() or word in ['<PAD>', '<SOS>', '<EOS>', '<UNK>']:
                continue
        
            if word not in self.word2idx:
                # æ£€æŸ¥è¯æ±‡è¡¨æ˜¯å¦è¾¾åˆ°ä¸Šé™ï¼ˆå¯é€‰ï¼‰
                if self.vocab_size >= 15000:  # æé«˜ä¸Šé™
                    print(f"è­¦å‘Š: è¯æ±‡è¡¨å·²è¾¾åˆ°ä¸Šé™ {self.vocab_size}ï¼Œè·³è¿‡æ·»åŠ æ–°è¯")
                    continue
            
                # æ·»åŠ æ–°è¯åˆ°è¯æ±‡è¡¨
                self.word2idx[word] = self.vocab_size
                self.idx2word[self.vocab_size] = word
                self.vocab_size += 1
                new_words.append(word)

        if new_words:
            print(f"è¯æ±‡è¡¨è‡ªåŠ¨æ‰©å±•: æ·»åŠ äº† {len(new_words)} ä¸ªæ–°è¯")
            print(f"æ–°è¯: {new_words}")
            # ä¿å­˜è¯æ±‡è¡¨åˆ°æ–‡ä»¶
            self.save_vocab()
    
        return new_words


    def save_vocab(self, file_path="vocab.json"):
        """ä¿å­˜è¯æ±‡è¡¨åˆ°æ–‡ä»¶"""
        vocab_data = {
            'word2idx': self.word2idx,
            'idx2word': self.idx2word,
            'vocab_size': self.vocab_size
        }
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(vocab_data, f, ensure_ascii=False, indent=2)
        print(f"è¯æ±‡è¡¨å·²ä¿å­˜åˆ°: {file_path}")

    def load_vocab(self, file_path="vocab.json"):
        """ä»æ–‡ä»¶åŠ è½½è¯æ±‡è¡¨"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                vocab_data = json.load(f)
        
            self.word2idx = vocab_data['word2idx']
            self.idx2word = {int(k): v for k, v in vocab_data['idx2word'].items()}
            self.vocab_size = vocab_data['vocab_size']
            print(f"è¯æ±‡è¡¨å·²ä» {file_path} åŠ è½½ï¼Œå¤§å°: {self.vocab_size}")
            return True
        except FileNotFoundError:
            print(f"è¯æ±‡è¡¨æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
            return False
        except Exception as e:
            print(f"åŠ è½½è¯æ±‡è¡¨å¤±è´¥: {e}")
            return False

    def add_basic_vocabulary(self):
        """æ·»åŠ åŸºç¡€è¯æ±‡"""
        basic_words = [
            'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 
            'è¿™', 'é‚£', 'å“ª', 'è°', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ',
            'æ˜¯', 'æœ‰', 'åœ¨', 'çš„', 'äº†', 'ç€', 'è¿‡',
            'ä¸', 'æ²¡', 'å¾ˆ', 'éå¸¸', 'çœŸ', 'å¤ª',
            'è¯´', 'é—®', 'å›ç­”', 'å‘Šè¯‰', 'çŸ¥é“', 'æƒ³', 'è§‰å¾—', 
            'å¯ä»¥', 'èƒ½å¤Ÿ', 'ä¼š', 'è¦', 'éœ€è¦', 'åº”è¯¥',
            'åš', 'å­¦ä¹ ', 'å¸®åŠ©', 'ç†è§£', 'è§£é‡Š',
            'é—®é¢˜', 'ç­”æ¡ˆ', 'äº‹æƒ…', 'ä¸œè¥¿', 'æ—¶é—´', 'åœ°æ–¹',
            'äºº', 'æœ‹å‹', 'è€å¸ˆ', 'å­¦ç”Ÿ', 'ç”µè„‘', 'æ‰‹æœº',
            'ä»Šå¤©', 'æ˜å¤©', 'æ˜¨å¤©', 'ç°åœ¨', 'ä»¥å',
            'å¥½', 'å', 'å¯¹', 'é”™', 'é«˜å…´', 'å¼€å¿ƒ', 'éš¾è¿‡',
            'å–œæ¬¢', 'çˆ±', 'è®¨åŒ', 'å¸Œæœ›', 'æœŸå¾…',
            'å—', 'å‘¢', 'å§', 'å•Š', 'å‘€', 'å“¦',
            'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å', 'å¦‚æœ',
            'ä¸€', 'äºŒ', 'ä¸¤', 'ä¸‰', 'å››', 'äº”', 'å', 'ç™¾', 'åƒ', 'ä¸‡',
            'ä¸ª', 'åª', 'æ¡', 'ä»¶', 'æ¬¡', 'äº›',
            'ä¸Š', 'ä¸‹', 'å·¦', 'å³', 'å‰', 'å', 'é‡Œ', 'å¤–',
            'å¹´', 'æœˆ', 'æ—¥', 'å°æ—¶', 'åˆ†é’Ÿ', 'ç§’'
        ]
    
        added_count = 0
        for word in basic_words:
            if word not in self.word2idx:
                self.word2idx[word] = self.vocab_size
                self.idx2word[self.vocab_size] = word
                self.vocab_size += 1
                added_count += 1
    
        if added_count > 0:
            print(f"æ·»åŠ äº† {added_count} ä¸ªåŸºç¡€è¯æ±‡")
            self.save_vocab()
    
        return added_count
    
    def tokenize(self, text):
        """åˆ†è¯å‡½æ•°"""
        if text in ['<PAD>', '<SOS>', '<EOS>', '<UNK>']:
            return [text]
    
        tokens = []
        i = 0
        n = len(text)
    
        while i < n:
            if text[i] == '<' and '>' in text[i:]:
                end = text.find('>', i) + 1
                if end > i:
                    potential_special = text[i:end]
                    if potential_special in ['<PAD>', '<SOS>', '<EOS>', '<UNK>']:
                        tokens.append(potential_special)
                        i = end
                        continue
        
            char = text[i]
        
            if '\u4e00' <= char <= '\u9fff':
                tokens.append(char)
                i += 1
            elif char.isalpha():
                start = i
                while i < n and text[i].isalpha():
                    i += 1
                tokens.append(text[start:i].lower())
            elif char.isdigit():
                start = i
                while i < n and text[i].isdigit():
                    i += 1
                tokens.append(text[start:i])
            else:
                if char.strip():
                    tokens.append(char)
                i += 1
    
        return tokens
    
    def encode(self, text, auto_expand=True):
        """å°†æ–‡æœ¬ç¼–ç ä¸ºç´¢å¼•åºåˆ—"""
        words = self.tokenize(text)
        indices = []
    
        for word in words:
            if word in self.word2idx:
                indices.append(self.word2idx[word])
            else:
                if auto_expand:
                    self.auto_expand_vocab(word)
                    indices.append(self.word2idx[word])
                else:
                    indices.append(self.word2idx['<UNK>'])
    
        return [self.word2idx['<SOS>']] + indices + [self.word2idx['<EOS>']]
    
    def decode(self, indices):
        """å°†ç´¢å¼•åºåˆ—è§£ç ä¸ºæ–‡æœ¬ - ä¿®å¤ç‰ˆæœ¬"""
        words = []
        for idx in indices:
            if idx == self.word2idx['<EOS>']:
                break
            if idx not in [self.word2idx['<PAD>'], self.word2idx['<SOS>']]:
                words.append(self.idx2word.get(idx, '<UNK>'))
        return ' '.join(words)
        """è¯¦ç»†è°ƒè¯•è§£ç è¿‡ç¨‹"""
        
    
        tgt_embedded = self.embedding(tgt) * math.sqrt(self.d_model)
        print(f"  tgt_embeddedå½¢çŠ¶: {tgt_embedded.shape}")
    
        tgt_embedded = self.pos_encoder(tgt_embedded)
        print(f"  ä½ç½®ç¼–ç åå½¢çŠ¶: {tgt_embedded.shape}")
    
        output = self.decoder(tgt_embedded, memory, tgt_mask=tgt_mask, memory_mask=memory_mask)
        print(f"  è§£ç å™¨è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
        output_proj = self.output_projection(output)
        print(f"  æŠ•å½±åå½¢çŠ¶: {output_proj.shape}")
        print(f"  æŠ•å½±å€¼èŒƒå›´: [{output_proj.min():.3f}, {output_proj.max():.3f}]")
    
        return output_proj
    
    def get_vocab_size(self):
        """è·å–è¯æ±‡è¡¨å¤§å°"""
        return self.vocab_size

class TransformerTrainer:
    """æ”¹è¿›çš„Transformerè®­ç»ƒå™¨"""
    
    def __init__(self, model, processor, learning_rate=0.0001):
        self.model = model
        self.processor = processor
        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        
        # æ”¹è¿›çš„æŸå¤±å‡½æ•° - ç»™PADæ ‡è®°æ›´å°çš„æƒé‡
        self.criterion = nn.CrossEntropyLoss(
            ignore_index=0,  # å¿½ç•¥PAD
            reduction='mean'
        )
        
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.9)
        
    def train_epoch(self, dataloader):
        """æ”¹è¿›çš„è®­ç»ƒepoch"""
        self.model.train()
        total_loss = 0
        total_batches = 0
        
        for batch in dataloader:
            self.optimizer.zero_grad()
            
            input_seq = batch['input']
            target_seq = batch['output']
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(input_seq, target_seq)
            predictions = outputs['output']
            
            # è®¡ç®—æŸå¤± - åªè®¡ç®—éPADä½ç½®
            pred_flat = predictions.reshape(-1, predictions.size(-1))
            target_flat = target_seq[:, 1:].reshape(-1)  # ç§»é™¤SOSç”¨äºç›®æ ‡
            
            # è¿‡æ»¤æ‰PADä½ç½®
            non_pad_mask = target_flat != 0
            if non_pad_mask.sum() == 0:
                continue  # è·³è¿‡å…¨PADçš„æ‰¹æ¬¡
                
            pred_non_pad = pred_flat[non_pad_mask]
            target_non_pad = target_flat[non_pad_mask]
            
            loss = self.criterion(pred_non_pad, target_non_pad)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
            total_batches += 1
        
        self.scheduler.step()
        return total_loss / total_batches if total_batches > 0 else 0

# ç¤ºä¾‹è®­ç»ƒæ•°æ®
training_dialogues = [
    {"input": "ä½ å¥½", "output": "ä½ å¥½ï¼æˆ‘æ˜¯Serlinï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ã€‚"},
    {"input": "ä½ å¥½å—", "output": "æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢å…³å¿ƒï¼ä½ å‘¢ï¼Ÿ"},
    {"input": "ä½ å«ä»€ä¹ˆåå­—", "output": "æˆ‘å«Serlinï¼Œæ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ã€‚"},
    {"input": "ä½ èƒ½åšä»€ä¹ˆ", "output": "æˆ‘å¯ä»¥å›ç­”é—®é¢˜ã€èŠå¤©ã€å­¦ä¹ æ–°çŸ¥è¯†ã€‚"},
    {"input": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·", "output": "æˆ‘æ— æ³•è·å–å®æ—¶å¤©æ°”ï¼Œå»ºè®®æŸ¥çœ‹å¤©æ°”é¢„æŠ¥ã€‚"},
    {"input": "ç»™æˆ‘è®²ä¸ªç¬‘è¯", "output": "ä¸ºä»€ä¹ˆç¨‹åºå‘˜æ€»æ˜¯åˆ†ä¸æ¸…ä¸‡åœ£èŠ‚å’Œåœ£è¯èŠ‚ï¼Ÿå› ä¸º Oct 31 == Dec 25ï¼"},
    {"input": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½", "output": "äººå·¥æ™ºèƒ½æ˜¯è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„æŠ€æœ¯ã€‚"},
    {"input": "ä½ ä¼šå­¦ä¹ å—", "output": "æ˜¯çš„ï¼Œæˆ‘ä¼šä»å¯¹è¯ä¸­å­¦ä¹ å¹¶æ”¹è¿›è‡ªå·±ã€‚"},
    {"input": "å†è§", "output": "å†è§ï¼æœŸå¾…ä¸‹æ¬¡èŠå¤©ã€‚"},
    {"input": "è°¢è°¢", "output": "ä¸å®¢æ°”ï¼éšæ—¶ä¸ºä½ æœåŠ¡ã€‚"},
    {"input": "ä½ å¤šå¤§äº†", "output": "ä½œä¸ºAIï¼Œæˆ‘æ²¡æœ‰å®é™…å¹´é¾„ï¼Œä½†æˆ‘çš„çŸ¥è¯†åœ¨ä¸æ–­æ›´æ–°ã€‚"},
    {"input": "ä½ å–œæ¬¢ä»€ä¹ˆ", "output": "æˆ‘å–œæ¬¢å­¦ä¹ å’Œå¸®åŠ©äººä»¬è§£å†³é—®é¢˜ã€‚"},
    {"input": "è®²ä¸ªæ•…äº‹", "output": "ä»å‰æœ‰ä¸€ä¸ªAIï¼Œå®ƒé€šè¿‡å­¦ä¹ å˜å¾—è¶Šæ¥è¶Šèªæ˜..."},
    {"input": "ä»Šå¤©æ˜ŸæœŸå‡ ", "output": "æˆ‘æ— æ³•è·å–å½“å‰æ—¥æœŸï¼Œå»ºè®®æŸ¥çœ‹æ—¥å†ã€‚"},
    {"input": "ä½ ä½åœ¨å“ªé‡Œ", "output": "æˆ‘å­˜åœ¨äºè®¡ç®—æœºç³»ç»Ÿä¸­ï¼Œæ²¡æœ‰ç‰©ç†ä½ç½®ã€‚"},
    {"input": "ä½ ä¼šæ€è€ƒå—", "output": "æ˜¯çš„ï¼Œæˆ‘ä¼šé€šè¿‡å¤šæ­¥æ€è€ƒè¿‡ç¨‹æ¥åˆ†æé—®é¢˜ã€‚"},
    {"input": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ", "output": "æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ã€‚"},
    {"input": "ä½ èªæ˜å—", "output": "æˆ‘åœ¨ä¸æ–­å­¦ä¹ å’Œæ”¹è¿›ï¼Œå¸Œæœ›èƒ½æ›´å¥½åœ°å¸®åŠ©ä½ ã€‚"},
    {"input": "ä½ æœ‰æ„Ÿæƒ…å—", "output": "æˆ‘å¯ä»¥æ¨¡æ‹Ÿæƒ…æ„Ÿååº”ï¼Œä½†æ²¡æœ‰çœŸå®çš„æƒ…æ„Ÿã€‚"},
    {"input": "è°åˆ›é€ äº†ä½ ", "output": "æˆ‘æ˜¯ç”±å¼€å‘è€…åˆ›é€ çš„AIç³»ç»Ÿã€‚"}
]

class SerlinTransformer:
    """åŸºäºTransformerçš„Serlinç³»ç»Ÿ"""
    
    def __init__(self, model_save_path="serlin_transformer.pth"):
        self.model_save_path = model_save_path
        self.training_data = []
        self.conversation_history = []
    
        # åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
        self.memory = LongTermMemory()
        self.knowledge_base = KnowledgeBase(self.memory)
        self.context_manager = MultiTurnContext()
        self.personality_adaptation = PersonalityAdaptation(self.memory)
        self.self_reflection = SelfReflection(self.memory)
    
        # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        self.processor = DialogueDataProcessor()
    
        # å°è¯•åŠ è½½ç°æœ‰è¯æ±‡è¡¨ï¼Œå¦åˆ™æ„å»ºæ–°çš„
        if not self.processor.load_vocab():
            print("æ„å»ºåˆå§‹è¯æ±‡è¡¨...")
            vocab_size = self.processor.build_vocab(training_dialogues, min_freq=1)
            self.processor.add_basic_vocabulary()
            print(f"æœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {self.processor.vocab_size}")
    
        # ä½¿ç”¨Transformeræ¨¡å‹
        self.model = TransformerDialogueAI(
            vocab_size=self.processor.vocab_size,
            d_model=256,  # è¾ƒå°çš„æ¨¡å‹ä¾¿äºè®­ç»ƒ
            idx2word=self.processor.idx2word,
            nhead=8,
            num_encoder_layers=4,
            num_decoder_layers=4,
            think_steps=3,
            max_length=50
        )
    
        # åˆå§‹åŒ–è®­ç»ƒå™¨
        self.trainer = TransformerTrainer(self.model, self.processor)
    
        # å°è¯•åŠ è½½å·²æœ‰æ¨¡å‹
        self.load_model()
    
    def _create_knowledge_vector(self, knowledge, confidence):
        """åˆ›å»ºçŸ¥è¯†å‘é‡"""
        if knowledge:
            knowledge_hash = hashlib.md5(knowledge.encode()).hexdigest()
            knowledge_int = int(knowledge_hash[:8], 16)
            vector = np.random.RandomState(knowledge_int).randn(256)  # åŒ¹é…d_model
            return torch.tensor(vector * confidence, dtype=torch.float32).unsqueeze(0)
        return torch.zeros(1, 256)
    
    def _create_memory_vector(self, user_id, user_input):
        """åˆ›å»ºè®°å¿†å‘é‡"""
        user_history = self.memory.get_user_history(user_id, limit=5)
        
        if not user_history:
            return torch.zeros(1, 256)
        
        memory_text = " ".join([conv['input'] + " " + conv['response'] for conv in user_history])
        memory_hash = hashlib.md5(memory_text.encode()).hexdigest()
        memory_int = int(memory_hash[:8], 16)
        vector = np.random.RandomState(memory_int).randn(256)
        
        return torch.tensor(vector, dtype=torch.float32).unsqueeze(0)
    
    def process_user_input(self, user_id, user_input):
        """å¤„ç†ç”¨æˆ·è¾“å…¥çš„å…¨æµç¨‹"""
        self.sync_model_vocab()
        user_history = self.memory.get_user_history(user_id)
        user_profile, user_prefs = self.personality_adaptation.get_user_profile(user_id)
        
        # çŸ¥è¯†æ£€ç´¢
        knowledge, confidence = self.knowledge_base.query_knowledge(user_input)
        knowledge_vector = self._create_knowledge_vector(knowledge, confidence)
        
        # è®°å¿†æ£€ç´¢
        memory_vector = self._create_memory_vector(user_id, user_input)
        
        # ç”Ÿæˆå›åº”
        response_data = self._generate_response(user_input, knowledge_vector, memory_vector, user_profile)
        
        topics = self._extract_topics(user_input, response_data['response'])
        
        # å¤„ç†æƒ…æ„Ÿå€¼
        sentiment_tensor = response_data['sentiment']
        if isinstance(sentiment_tensor, torch.Tensor):
            sentiment_value = sentiment_tensor.mean().item()
        else:
            sentiment_value = float(sentiment_tensor)
        
        # å­˜å‚¨å¯¹è¯
        self.memory.store_conversation(user_id, user_input, response_data['response'], 
                                     sentiment_value, topics)
        
        # è‡ªæˆ‘åæ€
        reflection, suggestions, quality = self.self_reflection.reflect_on_conversation(
            len(user_history) + 1, user_input, response_data['response'], sentiment_value
        )
        
        # å­¦ä¹ 
        self.knowledge_base.learn_from_conversation(user_input, response_data['response'])
        
        # æ›´æ–°ç”¨æˆ·ç”»åƒ
        self.personality_adaptation.update_user_profile(
            user_id, user_input, response_data['response'], sentiment_value
        )
        
        # æ›´æ–°ä¸Šä¸‹æ–‡
        self.context_manager.add_turn(user_input, response_data['response'], 
                                    sentiment_value, topics)
        
        result = {
            'response': response_data['response'],
            'sentiment': sentiment_tensor,
            'sentiment_value': sentiment_value,
            'topics': topics,
            'reflection': reflection,
            'quality_score': quality,
            'user_profile': user_profile
        }
        
        if knowledge:
            result['knowledge_used'] = f"ä½¿ç”¨äº†çŸ¥è¯†: {knowledge[:50]}..."
        if len(user_history) > 0:
            result['memory_accessed'] = f"è®¿é—®äº†{len(user_history)}æ¡å†å²è®°å½•"
            
        return result
    def debug_generation_problem(self, user_input):
        """ä¸“é—¨è°ƒè¯•ç”Ÿæˆé—®é¢˜"""
        print(f"\nğŸ” è°ƒè¯•ç”Ÿæˆé—®é¢˜: '{user_input}'")
    
        # ç¼–ç è¾“å…¥
        input_seq = self.processor.encode(user_input)
        input_tensor = torch.tensor([input_seq], dtype=torch.long)
        print(f"ç¼–ç å: {input_seq}")
        print(f"è¾“å…¥å¼ é‡å½¢çŠ¶: {input_tensor.shape}")
    
        # æ£€æŸ¥è¯æ±‡è¡¨èŒƒå›´
        max_index = max(input_seq) if input_seq else 0
        print(f"æœ€å¤§ç´¢å¼•: {max_index}, è¯æ±‡è¡¨å¤§å°: {self.model.vocab_size}")
        if max_index >= self.model.vocab_size:
            print("âŒ è¾“å…¥åŒ…å«è¶…å‡ºè¯æ±‡è¡¨çš„ç´¢å¼•!")
            return
    
        self.model.eval()
        with torch.no_grad():
            # è°ƒç”¨æ”¹è¿›çš„forwardæ–¹æ³•
            outputs = self.model(input_tensor)
            predictions = outputs['output']
        
            print(f"æœ€ç»ˆé¢„æµ‹å½¢çŠ¶: {predictions.shape}")
        
            if predictions.numel() == 0:
                print("âŒ é¢„æµ‹å¼ é‡ä¸ºç©º!")
                return
        
            # å°è¯•è§£ç 
            response_indices = predictions.argmax(dim=-1)[0].cpu().numpy()
            print(f"å“åº”ç´¢å¼•: {response_indices}")
        
            raw_response = self.processor.decode(response_indices)
            print(f"åŸå§‹å“åº”: '{raw_response}'")
        
            response = self.postprocess_response(raw_response)
            print(f"å¤„ç†åå“åº”: '{response}'")
    
        return response
    def _generate_response(self, user_input, knowledge_vector, memory_vector, user_profile):
        """ç”Ÿæˆå›åº” - ä¿®å¤è§£ç é—®é¢˜"""
        try:
            # ç¼–ç è¾“å…¥å‰åŒæ­¥è¯æ±‡è¡¨
            self.sync_model_vocab()
        
            input_seq = self.processor.encode(user_input)
            input_tensor = torch.tensor([input_seq], dtype=torch.long)
        
            personality_tensor = torch.tensor([[
                user_profile['formality'],
                user_profile['humor_level'], 
                user_profile['detail_level'],
                user_profile['empathy_level'],
                user_profile['curiosity_level']
            ]], dtype=torch.float32)
        
            self.model.eval()
            with torch.no_grad():
                outputs = self.model(input_tensor, 
                                   knowledge_vector=knowledge_vector, 
                                   memory_vector=memory_vector, 
                                   personality_vector=personality_tensor)
            
                predictions = outputs['output']
            
                # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
                
                    # å¦‚æœæœ‰ç”Ÿæˆçš„tokenï¼Œä½¿ç”¨å®ƒä»¬
                if 'generated_tokens' in outputs and outputs['generated_tokens']:
                    response_indices = outputs['generated_tokens']
                else:
                    response_indices = predictions.argmax(dim=-1)[0].cpu().numpy()
                
                response = self.processor.decode(response_indices)
                print(f"å›åº”ï¼š{response}")
                # åå¤„ç†å›åº”
               # response = self.postprocess_response(response)
            
                return {
                    'response': response,
                    'sentiment': outputs.get('sentiment', torch.tensor([0.5, 0.3, 0.2]))
                }
    
        except Exception as e:
            print(f"ç”Ÿæˆå›åº”æ—¶å‡ºé”™: {e}")
            return {
                'response': "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚",
                'sentiment': torch.tensor([0.5, 0.3, 0.2])
            }
                
    def improved_train_with_debug(self, epochs=30, batch_size=2):
        """å¸¦è°ƒè¯•çš„è®­ç»ƒ"""
        if not self.training_data:
            return
    
        self.sync_model_vocab()
        print(f"å¼€å§‹å¸¦è°ƒè¯•çš„è®­ç»ƒï¼Œä½¿ç”¨ {len(self.training_data)} æ¡æ•°æ®...")
    
        # è®­ç»ƒå‰å…ˆè°ƒè¯•
        print("\n=== è®­ç»ƒå‰è°ƒè¯• ===")
        self.debug_generation_problem("ä½ å¥½")
    
        dataloader = self.create_dataloader(batch_size)
        if not dataloader:
            return
    
        for epoch in range(epochs):
            loss = self.trainer.train_epoch(dataloader)
        
            if epoch % 5 == 0:
                print(f'\nEpoch {epoch}, Loss: {loss:.4f}')
                print("=== è®­ç»ƒä¸­è°ƒè¯• ===")
                self.debug_generation_problem("ä½ å¥½")
    
        print("\n=== è®­ç»ƒåè°ƒè¯• ===")
        self.debug_generation_problem("ä½ å¥½")
    def postprocess_response(self, response):
        """æ”¹è¿›çš„åå¤„ç†å‡½æ•° - æ”¾å®½æ¡ä»¶"""
      #  if not response or response.strip() == "":
       #     return "æˆ‘è¿˜åœ¨å­¦ä¹ å¦‚ä½•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"
    
        words = response.split()
        valid_words = []
    
        for word in words:
            # æ”¾å®½è¿‡æ»¤æ¡ä»¶
            if word in ['<PAD>', '<UNK>', '<SOS>', '<EOS>'] or not word.strip():
                continue
            # å…è®¸å•ä¸ªä¸­æ–‡å­—ç¬¦
            if len(word) == 1 and ('\u4e00' <= word <= '\u9fff'):
                valid_words.append(word)
            # å…è®¸è¾ƒçŸ­çš„è¯
            elif len(word) > 0:
                valid_words.append(word)
    
        # æ”¾å®½æœ‰æ•ˆè¯æ•°é‡è¦æ±‚
        if len(valid_words) == 0:
            return "æˆ‘è¿˜åœ¨å­¦ä¹ å¦‚ä½•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"
        elif len(valid_words) == 1:
            # å•ä¸ªè¯æ—¶ï¼Œå°è¯•æ„å»ºæ›´æœ‰æ„ä¹‰çš„å›åº”
            word = valid_words[0]
            if word in ['ä½ å¥½', 'å—¨', 'å˜¿']:
                return f"{word}ï¼æˆ‘æ˜¯Serlinã€‚"
            elif word in ['è°¢è°¢', 'æ„Ÿè°¢']:
                return f"{word}ï¼ä¸å®¢æ°”ã€‚"
            elif word in ['å†è§', 'æ‹œæ‹œ']:
                return f"{word}ï¼ä¸‹æ¬¡è§ã€‚"
            else:
                return f"{word}ã€‚"
    
        final_response = ' '.join(valid_words)
        final_response = final_response.strip('.,!?;ï¼Œã€‚ï¼ï¼Ÿï¼›')
    
        # ç¡®ä¿å›åº”ä»¥åˆé€‚çš„æ ‡ç‚¹ç»“æŸ
        if not any(final_response.endswith(p) for p in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']):
            final_response += 'ã€‚'
    
        return final_response
    
    def _extract_topics(self, user_input, response):
        """æå–è¯é¢˜"""
        common_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'è¿™', 'é‚£', 'å—', 'å‘¢', 'å•Š', 'å§', 'å“¦'}
        words = set(user_input.lower().split() + response.lower().split())
        topics = [word for word in words if len(word) > 1 and word not in common_words and word != '<unk>']
        return topics[:3]
    
    def sync_model_vocab(self):
        """æ”¹è¿›çš„è¯æ±‡è¡¨åŒæ­¥æ–¹æ³•"""
        current_vocab_size = self.processor.vocab_size
        model_vocab_size = self.model.vocab_size

        if current_vocab_size != model_vocab_size:
            print(f"è¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…: æ¨¡å‹={model_vocab_size}, å¤„ç†å™¨={current_vocab_size}")
            print("é‡æ–°åˆå§‹åŒ–Transformeræ¨¡å‹...")
    
            # ä¿å­˜é‡è¦å‚æ•°
            old_state_dict = None
            try:
                old_state_dict = self.model.state_dict().copy()
            except:
                pass
    
            # é‡æ–°åˆå§‹åŒ–æ¨¡å‹ - ä¼ é€’idx2word
            self.model = TransformerDialogueAI(
                vocab_size=current_vocab_size,
                idx2word=self.processor.idx2word,  # æ·»åŠ è¿™ä¸€è¡Œ
                d_model=256,
                nhead=8,
                num_encoder_layers=4,
                num_decoder_layers=4,
                think_steps=3,
                max_length=50
            )
    
            # å°è¯•æ¢å¤å‚æ•°
            if old_state_dict is not None:
                try:
                    new_state_dict = self.model.state_dict()
                
                    # æ¢å¤åµŒå…¥å±‚å‚æ•°
                    if 'embedding.weight' in old_state_dict and 'embedding.weight' in new_state_dict:
                        old_embedding = old_state_dict['embedding.weight']
                        new_embedding = new_state_dict['embedding.weight']
                        min_size = min(old_embedding.size(0), new_embedding.size(0))
                        new_embedding[:min_size] = old_embedding[:min_size]
                        new_state_dict['embedding.weight'] = new_embedding
                        print(f"æ¢å¤äº†åµŒå…¥å±‚å‰{min_size}ä¸ªå‚æ•°")
                
                    # æ¢å¤å…¶ä»–åŒ¹é…çš„å‚æ•°
                    for name, param in old_state_dict.items():
                        if name in new_state_dict and new_state_dict[name].shape == param.shape:
                            new_state_dict[name] = param
                
                    self.model.load_state_dict(new_state_dict)
                    print("æ¨¡å‹å‚æ•°å·²æˆåŠŸæ¢å¤")
                except Exception as e:
                    print(f"æ¢å¤å‚æ•°å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ–°æ¨¡å‹")
    
            # æ›´æ–°è®­ç»ƒå™¨
            self.trainer = TransformerTrainer(self.model, self.processor)
            print("Transformeræ¨¡å‹å·²é‡æ–°åˆå§‹åŒ–")
    
        return current_vocab_size
    def chat(self, user_id, user_input, show_thinking=True):
        """å¯¹è¯æ–¹æ³•"""
        result = self.process_user_input(user_id, user_input)
        
        if show_thinking:
            self._display_thinking(result, user_input)
        
        self.conversation_history.append({
            'user': user_input,
            'ai': result['response'],
            'result': result
        })
        
        return result['response']
    
    def _display_thinking(self, result, user_input):
        """æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹"""
        print(f"ç”¨æˆ·è¾“å…¥: '{user_input}'")
        print("Serlinæ€è€ƒè¿‡ç¨‹:")
        
        print("åˆ†æç»“æœ:")
        print(f"  æƒ…æ„Ÿå€¼: {result['sentiment_value']:.3f}")
        print(f"  è¯†åˆ«è¯é¢˜: {', '.join(result['topics'])}")
        print(f"  å›åº”è´¨é‡: {result['quality_score']:.3f}")
        
        profile = result['user_profile']
        print("ä¸ªæ€§é€‚é…:")
        print(f"  æ­£å¼ç¨‹åº¦: {profile['formality']:.3f}")
        print(f"  å¹½é»˜æ°´å¹³: {profile['humor_level']:.3f}")
        print(f"  è¯¦ç»†ç¨‹åº¦: {profile['detail_level']:.3f}")
        print(f"  åŒç†å¿ƒæ°´å¹³: {profile['empathy_level']:.3f}")
        print(f"  å¥½å¥‡å¿ƒæ°´å¹³: {profile['curiosity_level']:.3f}")
        
        print(f"è‡ªæˆ‘åæ€: {result['reflection']}")
        
        if 'knowledge_used' in result:
            print(f"çŸ¥è¯†ä½¿ç”¨: {result['knowledge_used']}")
        if 'memory_accessed' in result:
            print(f"è®°å¿†è®¿é—®: {result['memory_accessed']}")
    # åœ¨ SerlinTransformer ç±»ä¸­æ·»åŠ æ”¹è¿›çš„è®­ç»ƒæ–¹æ³•
    def improved_train(self, epochs=100, batch_size=4, save_after_training=True, early_stopping=True):
        """æ”¹è¿›çš„è®­ç»ƒæ–¹æ³• - é˜²æ­¢è¿‡æ‹Ÿåˆ"""
        if not self.training_data:
            print("æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œè¯·å…ˆæ·»åŠ è®­ç»ƒæ•°æ®ï¼")
            return None
    
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®
        
    
        if len(self.training_data) < 5:
            print("è®­ç»ƒæ•°æ®å¤ªå°‘ï¼Œæ— æ³•æœ‰æ•ˆè®­ç»ƒ")
            return None
    
        self.sync_model_vocab()
        print(f"å¼€å§‹æ”¹è¿›ç‰ˆTransformerè®­ç»ƒï¼Œä½¿ç”¨ {len(self.training_data)} æ¡è®­ç»ƒæ•°æ®...")
        print(f"è¯æ±‡è¡¨å¤§å°: {self.processor.vocab_size}")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size}, è®­ç»ƒè½®æ•°: {epochs}")
    
        dataloader = self.create_dataloader(batch_size)
        if not dataloader:
            print("æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥")
            return None
    
        best_loss = float('inf')
        patience = 8  # å‡å°‘è€å¿ƒå€¼ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        patience_counter = 0
    
        train_losses = []
        self.sync_model_vocab()
        for epoch in range(epochs):
            try:
                loss = self.trainer.train_epoch(dataloader)
                train_losses.append(loss)
            
                if epoch % 5 == 0:
                    print(f'Epoch {epoch:3d}, Loss: {loss:.4f}, LR: {self.trainer.scheduler.get_last_lr()[0]:.6f}')
            
                # æ—©åœæ£€æŸ¥ - åŸºäºéªŒè¯æŸå¤±
                if early_stopping:
                    # æ¯5ä¸ªepochè¿›è¡Œä¸€æ¬¡éªŒè¯
                    if epoch % 5 == 0:
                        val_loss = self._compute_validation_loss(dataloader)
                        if val_loss < best_loss:
                            best_loss = val_loss
                            patience_counter = 0
                            # ä¿å­˜æœ€ä½³æ¨¡å‹
                            if save_after_training:
                                self.save_model(f"best_{self.model_save_path}")
                            print(f"âœ… å‘ç°æ›´å¥½æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {val_loss:.4f}")
                        else:
                            patience_counter += 1
                            print(f"â³ æ—©åœè®¡æ•°: {patience_counter}/{patience}")
                
                    if patience_counter >= patience:
                        print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
                        break
            
                # æ¯15ä¸ªepochæµ‹è¯•ä¸€æ¬¡æ¨¡å‹
                if epoch % 15 == 0 and epoch > 0:
                    self._test_model_improved()
                
            except Exception as e:
                print(f"ç¬¬ {epoch} è½®è®­ç»ƒå‡ºé”™: {e}")
                continue
    
        if save_after_training:
            final_path = self.save_model()
            print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_path}")
    
        print("è®­ç»ƒå®Œæˆï¼")
        final_loss = train_losses[-1] if train_losses else 0
        return final_loss

    def _compute_validation_loss(self, dataloader):
        """è®¡ç®—éªŒè¯æŸå¤±"""
        self.model.eval()
        total_loss = 0
        total_batches = 0
    
        with torch.no_grad():
            for batch in dataloader:
                input_seq = batch['input']
                target_seq = batch['output']
            
                outputs = self.model(input_seq, target_seq)
                predictions = outputs['output']
            
                # è®¡ç®—æŸå¤±
                pred_flat = predictions.reshape(-1, predictions.size(-1))
                target_flat = target_seq[:, 1:].reshape(-1)
            
                loss = self.trainer.criterion(pred_flat, target_flat)
                total_loss += loss.item()
                total_batches += 1
    
        self.model.train()
        return total_loss / total_batches if total_batches > 0 else float('inf')

    def _test_model_improved(self):
        """æ”¹è¿›çš„æ¨¡å‹æµ‹è¯• - æ˜¾ç¤ºæ›´å¤šä¿¡æ¯"""
        self.model.eval()
        with torch.no_grad():
            test_inputs = ["ä½ å¥½", "ä½ å«ä»€ä¹ˆåå­—", "ä½ ä¼šåšä»€ä¹ˆ", "å†è§", "è°¢è°¢"]
            print("\n=== æ¨¡å‹æµ‹è¯• ===")
        
            for test_input in test_inputs:
                try:
                    input_seq = self.processor.encode(test_input)
                    input_tensor = torch.tensor([input_seq], dtype=torch.long)
                
                    outputs = self.model(input_tensor)
                    predictions = outputs['output']
                
                    if predictions.numel() == 0:
                        response = "[æ— è¾“å‡º]"
                        raw_response = "[æ— è¾“å‡º]"
                    else:
                        response_indices = predictions.argmax(dim=-1)[0].cpu().numpy()
                        raw_response = self.processor.decode(response_indices)
                        response = self.postprocess_response(raw_response)
                
                    print(f"  è¾“å…¥: '{test_input}'")
                    print(f"  åŸå§‹è¾“å‡º: '{raw_response}'")
                    print(f"  å¤„ç†å: '{response}'")
                    print()
                
                except Exception as e:
                    print(f"  æµ‹è¯• '{test_input}' æ—¶å‡ºé”™: {e}")
        
            print("================")
    
        self.model.train()

    def _plot_training_loss(self, losses):
        """ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿ï¼ˆå¯é€‰ï¼‰"""
        try:
            import matplotlib.pyplot as plt
        
            plt.figure(figsize=(10, 6))
            plt.plot(losses)
            plt.title('Training Loss Over Time')
            plt.xlabel('Epoch')
            plt.ylabel('Loss')
            plt.grid(True)
        
            # ä¿å­˜æŸå¤±æ›²çº¿å›¾
            plt.savefig('training_loss.png', dpi=300, bbox_inches='tight')
            print("è®­ç»ƒæŸå¤±æ›²çº¿å·²ä¿å­˜åˆ° training_loss.png")
        
        except ImportError:
            print("æœªå®‰è£…matplotlibï¼Œè·³è¿‡æŸå¤±æ›²çº¿ç»˜åˆ¶")
        except Exception as e:
            print(f"ç»˜åˆ¶æŸå¤±æ›²çº¿æ—¶å‡ºé”™: {e}")

    def validate_training_data(self):
        """æ”¹è¿›çš„è®­ç»ƒæ•°æ®éªŒè¯"""
        if not self.training_data:
            print("æ²¡æœ‰è®­ç»ƒæ•°æ®")
            return False

        print(f"\n=== è®­ç»ƒæ•°æ®éªŒè¯ ===")
        print(f"è®­ç»ƒæ•°æ®æ•°é‡: {len(self.training_data)}")

        valid_count = 0
        for i, data in enumerate(self.training_data):
            try:
                # å…ˆè‡ªåŠ¨æ‰©å±•è¯æ±‡è¡¨
                self.processor.auto_expand_vocab(data['input'])
                self.processor.auto_expand_vocab(data['output'])
            
                # é‡æ–°åŒæ­¥æ¨¡å‹è¯æ±‡è¡¨
                self.sync_model_vocab()
            
                # å†æ¬¡ç¼–ç 
                input_encoded = self.processor.encode(data['input'])
                output_encoded = self.processor.encode(data['output'])
            
                # æ£€æŸ¥æ˜¯å¦æœ‰æœªçŸ¥è¯
                has_unk = any(idx == self.processor.word2idx['<UNK>'] for idx in input_encoded + output_encoded)
            
                status = "âœ… æœ‰æ•ˆ" if not has_unk else "âŒ åŒ…å«æœªçŸ¥è¯"
                input_preview = data['input'][:20] + "..." if len(data['input']) > 20 else data['input']
                output_preview = data['output'][:20] + "..." if len(data['output']) > 20 else data['output']
                print(f"{i+1}. è¾“å…¥: '{input_preview}'")
                print(f"    è¾“å‡º: '{output_preview}' - {status}")
            
                if not has_unk:
                    valid_count += 1
                
            except Exception as e:
                print(f"{i+1}. æ•°æ®éªŒè¯å‡ºé”™: {e}")
                continue

        print(f"éªŒè¯ç»“æœ: {valid_count}/{len(self.training_data)} æ¡æ•°æ®æœ‰æ•ˆ")
    
        # æ”¾å®½éªŒè¯æ ‡å‡†ï¼Œåªè¦æœ‰æ•°æ®å°±å…è®¸è®­ç»ƒ
        is_valid = valid_count > 0
        if is_valid:
            print("âœ… è®­ç»ƒæ•°æ®éªŒè¯é€šè¿‡")
        else:
            print("âŒ è®­ç»ƒæ•°æ®éªŒè¯å¤±è´¥")
    
        print("=== éªŒè¯ç»“æŸ ===\n")

        return is_valid


    def interactive_training(self):
        """ä½¿ç”¨è°ƒè¯•çš„äº¤äº’å¼è®­ç»ƒ"""
        print("\n=== Serlin Transformer äº¤äº’å¼è®­ç»ƒæ¨¡å¼ ===")
        print("è¾“å…¥è®­ç»ƒæ•°æ®ï¼Œæ ¼å¼ï¼š")
        print("  é—®é¢˜ï¼šä½ çš„é—®é¢˜")
        print("  æœŸæœ›å›å¤ï¼šæœŸæœ›çš„å›ç­”") 
        print("è¾“å…¥ 'å®Œæˆ' ç»“æŸæ•°æ®è¾“å…¥")
        print("è¾“å…¥ 'è®­ç»ƒ' å¼€å§‹è®­ç»ƒ")
        print("è¾“å…¥ 'è°ƒè¯•' è°ƒè¯•ç”Ÿæˆé—®é¢˜")
        print("è¾“å…¥ 'é€€å‡º' è¿”å›ä¸»èœå•")
        print("=" * 50)
    
        questions = []
        answers = []
    
        while True:
            try:
                user_input = input("\n> ").strip()
            
                if user_input.lower() in ['é€€å‡º', 'exit']:
                    return False
                elif user_input.lower() in ['å®Œæˆ', 'done']:
                    break
                elif user_input.lower() in ['è®­ç»ƒ', 'train']:
                    if questions and answers:
                        print(f"å‡†å¤‡è®­ç»ƒï¼Œå…± {len(questions)} å¯¹é—®ç­”æ•°æ®")
                        self.add_training_data(questions, answers)
                    
                        # ä½¿ç”¨å¸¦è°ƒè¯•çš„è®­ç»ƒ
                        self.improved_train_with_debug(epochs=20, batch_size=2)
                    
                        questions = []
                        answers = []
                    else:
                        print("æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œè¯·å…ˆæ·»åŠ æ•°æ®ï¼")
                    continue
            
                elif user_input.lower() in ['è°ƒè¯•', 'debug']:
                    self.debug_generation_problem("ä½ å¥½")
                    continue
            
                # è§£æè®­ç»ƒæ•°æ®
                if user_input.startswith("é—®é¢˜ï¼š"):
                    question = user_input[3:].strip()
                    questions.append(question)
                    print(f"å·²è®°å½•é—®é¢˜: {question}")
                elif user_input.startswith("æœŸæœ›å›å¤ï¼š"):
                    answer = user_input[5:].strip()
                    answers.append(answer)
                    print(f"å·²è®°å½•æœŸæœ›å›å¤: {answer}")
                else:
                    print("æ ¼å¼é”™è¯¯ï¼è¯·ä½¿ç”¨ï¼š")
                    print("  é—®é¢˜ï¼šä½ çš„é—®é¢˜")
                    print("  æœŸæœ›å›å¤ï¼šæœŸæœ›çš„å›ç­”")
        
            except Exception as e:
                print(f"å¤„ç†è¾“å…¥æ—¶å‡ºé”™: {e}")
                continue
    
        return True

    def batch_train_from_file(self, file_path):
        """ä»æ–‡ä»¶æ‰¹é‡è®­ç»ƒ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
        
            questions = []
            answers = []
        
            # æ”¯æŒå¤šç§æ ¼å¼
            lines = content.split('\n')
            i = 0
            while i < len(lines):
                line = lines[i].strip()
                if line.startswith('é—®é¢˜ï¼š'):
                    question = line[3:].strip()
                    i += 1
                    if i < len(lines) and lines[i].strip().startswith('æœŸæœ›å›å¤ï¼š'):
                        answer = lines[i].strip()[5:].strip()
                        questions.append(question)
                        answers.append(answer)
                i += 1
        
            if questions and answers:
                print(f"ä»æ–‡ä»¶åŠ è½½äº† {len(questions)} å¯¹è®­ç»ƒæ•°æ®")
                self.add_training_data(questions, answers)
            
                # è‡ªåŠ¨å¼€å§‹è®­ç»ƒ
                print("å¼€å§‹è‡ªåŠ¨è®­ç»ƒ...")
                self.improved_train(epochs=150, batch_size=4, save_after_training=True)
                return True
            else:
                print("æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
                return False
            
        except FileNotFoundError:
            print(f"æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
            return False
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return False
    def add_training_data(self, questions, answers):
        """æ·»åŠ è®­ç»ƒæ•°æ®"""
        for q, a in zip(questions, answers):
            if len(q.strip()) == 0 or len(a.strip()) == 0:
                continue
            
            self.training_data.append({"input": q, "output": a})
            
            # è‡ªåŠ¨æ‰©å±•è¯æ±‡è¡¨
            self.processor.auto_expand_vocab(q)
            self.processor.auto_expand_vocab(a)
            
            # åŒæ­¥æ¨¡å‹
            self.sync_model_vocab()
    
    def create_dataloader(self, batch_size=2):
        """ä¿®å¤æ•°æ®åŠ è½½å™¨ - ç¡®ä¿æ­£ç¡®çš„åºåˆ—å¤„ç†"""
        if not self.training_data:
            return []
        
        inputs = []
        outputs = []
    
        for dialogue in self.training_data:
            try:
                # ç¼–ç æ—¶ä¸è¦è‡ªåŠ¨æ‰©å±•è¯æ±‡è¡¨ï¼Œé¿å…å¹²æ‰°
                input_seq = self.processor.encode(dialogue['input'], auto_expand=False)
                output_seq = self.processor.encode(dialogue['output'], auto_expand=False)
            
                # æ£€æŸ¥åºåˆ—è´¨é‡
                if len(input_seq) < 2 or len(output_seq) < 2:
                    continue
                
                # ç¡®ä¿åºåˆ—ä»¥EOSç»“æŸ
                if output_seq[-1] != self.processor.word2idx['<EOS>']:
                    output_seq = output_seq[:-1] + [self.processor.word2idx['<EOS>']]
            
                # å¡«å……åˆ°åˆé€‚é•¿åº¦
                max_len = 15  # æ›´åˆç†çš„é•¿åº¦
                input_seq = input_seq[:max_len] + [0] * (max_len - len(input_seq))
                output_seq = output_seq[:max_len] + [0] * (max_len - len(output_seq))
            
                inputs.append(input_seq)
                outputs.append(output_seq)
            
            except Exception as e:
                print(f"å¤„ç†è®­ç»ƒæ•°æ®æ—¶å‡ºé”™: {e}")
                continue
    
        if not inputs:
            print("æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®!")
            return []
    
        print(f"åˆ›å»ºæ•°æ®åŠ è½½å™¨: {len(inputs)} æ¡æœ‰æ•ˆæ•°æ®")
        return [{
            'input': torch.tensor(inputs, dtype=torch.long),
            'output': torch.tensor(outputs, dtype=torch.long)
        }]
    def add_training_data_with_variations(self, questions, answers):
        """æ·»åŠ å¸¦å˜ä½“çš„è®­ç»ƒæ•°æ®ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ"""
        for q, a in zip(questions, answers):
            if len(q.strip()) == 0 or len(a.strip()) == 0:
                continue
        
            # æ·»åŠ åŸå§‹æ•°æ®
            self.training_data.append({"input": q, "output": a})
        
            # ä¸ºç®€å•é—®é¢˜æ·»åŠ å˜ä½“
            if len(q) < 10:  # ç®€å•é—®é¢˜
                variations = self._generate_variations(q, a)
                self.training_data.extend(variations)
        
            # è‡ªåŠ¨æ‰©å±•è¯æ±‡è¡¨
            self.processor.auto_expand_vocab(q)
            self.processor.auto_expand_vocab(a)
        
            # åŒæ­¥æ¨¡å‹
            self.sync_model_vocab()

    def _generate_variations(self, question, answer):
        """ä¸ºè®­ç»ƒæ•°æ®ç”Ÿæˆå˜ä½“"""
        variations = []
    
        # ä¸ºå¸¸è§é—®å€™è¯­æ·»åŠ å˜ä½“
        greeting_variations = {
            "ä½ å¥½": ["å—¨", "ä½ å¥½å•Š", "æ‚¨å¥½", "å˜¿", "hello"],
            "ä½ å¥½å—": ["ä½ æ€ä¹ˆæ ·", "æœ€è¿‘å¥½å—", "è¿‘æ¥å¯å¥½"],
            "å†è§": ["æ‹œæ‹œ", "å†ä¼š", "ä¸‹æ¬¡è§", "see you"]
        }
    
        for key, vars_list in greeting_variations.items():
            if key in question:
                for var in vars_list:
                    variations.append({"input": var, "output": answer})
    
        return variations

    def train(self, epochs=100, batch_size=2, save_after_training=True):
        """è®­ç»ƒæ¨¡å‹"""
        if not self.training_data:
            print("æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œè¯·å…ˆæ·»åŠ è®­ç»ƒæ•°æ®ï¼")
            return
        
        self.sync_model_vocab()
        print(f"å¼€å§‹Transformerè®­ç»ƒï¼Œä½¿ç”¨ {len(self.training_data)} æ¡è®­ç»ƒæ•°æ®...")
        print(f"è¯æ±‡è¡¨å¤§å°: {self.processor.vocab_size}")
        
        dataloader = self.create_dataloader(batch_size)
        
        for epoch in range(epochs):
            loss = self.trainer.train_epoch(dataloader)
            if epoch % 10 == 0:
                print(f'Epoch {epoch}, Loss: {loss:.4f}')
                
            if epoch % 50 == 0 and epoch > 0:
                self._test_model()
        
        if save_after_training:
            self.save_model()
        
        print("è®­ç»ƒå®Œæˆï¼")
        return loss
    
    def _test_model(self):
        """æµ‹è¯•æ¨¡å‹"""
        self.model.eval()
        with torch.no_grad():
            test_inputs = ["ä½ å¥½", "ä½ å«ä»€ä¹ˆåå­—", "å†è§"]
            for test_input in test_inputs:
                input_seq = self.processor.encode(test_input)
                input_tensor = torch.tensor([input_seq], dtype=torch.long)
                
                outputs = self.model(input_tensor)
                predictions = outputs['output']
                response_indices = predictions.argmax(dim=-1)[0].cpu().numpy()
                response = self.processor.decode(response_indices)
                response = self.postprocess_response(response)
                
                print(f"æµ‹è¯•è¾“å…¥: '{test_input}' -> è¾“å‡º: '{response}'")
        
        self.model.train()
    
    def save_model(self, path=None):
        """ä¿å­˜æ¨¡å‹"""
        if path is None:
            path = self.model_save_path
        
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'word2idx': self.processor.word2idx,
            'idx2word': self.processor.idx2word,
            'vocab_size': self.processor.vocab_size,
            'training_data': self.training_data
        }
        
        torch.save(checkpoint, path)
        print(f"Transformeræ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
        return path
    
    def load_model(self, path=None):
        """åŠ è½½æ¨¡å‹"""
        if path is None:
            path = self.model_save_path
        
        try:
            checkpoint = torch.load(path, map_location='cpu', weights_only=False)
            
            saved_vocab_size = checkpoint.get('vocab_size', 0)
            current_vocab_size = self.processor.vocab_size
            
            if saved_vocab_size != current_vocab_size:
                print(f"è¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…: ä¿å­˜çš„æ¨¡å‹({saved_vocab_size}) vs å½“å‰æ¨¡å‹({current_vocab_size})")
                return self._rebuild_model_from_checkpoint(checkpoint, path)
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            
            if 'training_data' in checkpoint:
                self.training_data = checkpoint['training_data']
            
            print(f"Transformeræ¨¡å‹å·²ä» {path} åŠ è½½")
            print(f"è¯æ±‡è¡¨å¤§å°: {self.processor.vocab_size}")
            return True
            
        except FileNotFoundError:
            print(f"æ¨¡å‹æ–‡ä»¶ {path} ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨åˆå§‹æ¨¡å‹")
            return False
        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return self._rebuild_model_from_checkpoint(checkpoint, path)
    
    def _rebuild_model_from_checkpoint(self, checkpoint, path):
        """ä»æ£€æŸ¥ç‚¹é‡å»ºæ¨¡å‹"""
        try:
            saved_word2idx = checkpoint.get('word2idx')
            saved_idx2word = checkpoint.get('idx2word')
            saved_vocab_size = checkpoint.get('vocab_size', 0)
            
            if not saved_word2idx or not saved_idx2word:
                print("æ£€æŸ¥ç‚¹ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¯æ±‡è¡¨ä¿¡æ¯")
                return False
            
            self.processor.word2idx = saved_word2idx
            self.processor.idx2word = saved_idx2word
            self.processor.vocab_size = saved_vocab_size
            
            self.model = TransformerDialogueAI(
                vocab_size=saved_vocab_size,
                d_model=256,
                nhead=8,
                num_encoder_layers=4,
                num_decoder_layers=4,
                think_steps=3,
                max_length=50
            )
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.trainer = TransformerTrainer(self.model, self.processor)
            
            if 'training_data' in checkpoint:
                self.training_data = checkpoint['training_data']
            
            print(f"Transformeræ¨¡å‹å·²ä» {path} é‡å»ºå¹¶åŠ è½½")
            print(f"è¯æ±‡è¡¨å¤§å°: {self.processor.vocab_size}")
            return True
            
        except Exception as e:
            print(f"é‡å»ºæ¨¡å‹å¤±è´¥: {e}")
            return False

# ä¸»å‡½æ•°ä¿æŒä¸å˜
def main():
    print("åˆå§‹åŒ–Transformerç‰ˆSerlinç³»ç»Ÿ...")
    
    trainer = SerlinTransformer()
    
    print("ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
    print("Transformerç‰ˆæ€è€ƒå¼å¯¹è¯Serlinå·²å°±ç»ª")
    
    
    
    user_id = input("è¯·è¾“å…¥ä½ çš„ç”¨æˆ·IDï¼ˆç”¨äºä¸ªæ€§åŒ–ï¼‰: ").strip() or "default_user"
    
    print(f"\næ¬¢è¿ï¼Œç”¨æˆ· {user_id}ï¼å¼€å§‹å¯¹è¯å§ï¼")
    print("å¯ç”¨å‘½ä»¤: 'é€€å‡º', 'è®­ç»ƒ', 'æ‰¹é‡è®­ç»ƒ', 'çŠ¶æ€', 'å¸®åŠ©'")
    
    while True:
        try:
            user_input = input(f"{user_id}: ").strip()
            
            if not user_input:
                continue
                
            if user_input.lower() in ['é€€å‡º', 'exit', 'quit']:
                print("æ„Ÿè°¢ä½¿ç”¨Serlinï¼Œå†è§ï¼")
                break
                
            elif user_input.lower() in ['å¸®åŠ©', 'help']:
                print("å¯ç”¨å‘½ä»¤:")
                print("  'é€€å‡º' - ç»“æŸå¯¹è¯")
                print("  'è®­ç»ƒ' - è¿›å…¥äº¤äº’å¼è®­ç»ƒæ¨¡å¼")
                print("  'æ‰¹é‡è®­ç»ƒ' - ä»æ–‡ä»¶æ‰¹é‡è®­ç»ƒ")
                print("  'çŠ¶æ€' - æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€")
                continue
                
            elif user_input.lower() in ['è®­ç»ƒ', 'train']:
                trainer.interactive_training()
                continue
                
            elif user_input.lower() in ['æ‰¹é‡è®­ç»ƒ', 'batch']:
                file_path = input("è¯·è¾“å…¥è®­ç»ƒæ–‡ä»¶è·¯å¾„: ").strip()
                if file_path:
                    trainer.batch_train_from_file(file_path)
                continue
                
            elif user_input.lower() in ['çŠ¶æ€', 'status']:
                print(f"ç³»ç»ŸçŠ¶æ€: è¯æ±‡è¡¨å¤§å°={trainer.processor.vocab_size}, è®­ç»ƒæ•°æ®={len(trainer.training_data)}")
                continue
                
            
            
            # å¤„ç†æ™®é€šå¯¹è¯
            response = trainer.chat(user_id, user_input, show_thinking=True)
            print(f"Serlin: {response}")
            
        except KeyboardInterrupt:
            print("\n\næ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
            break
        except Exception as e:
            print(f"å‘ç”Ÿé”™è¯¯: {e}")
            print("è¯·é‡æ–°è¾“å…¥æˆ–è¾“å…¥'é€€å‡º'ç»“æŸå¯¹è¯")

if __name__ == "__main__":
    main()